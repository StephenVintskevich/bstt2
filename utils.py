# -*- coding: utf-8 -*-
"""utils.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/175mT24gaUBdAjp77OQ6fLBBmORh71443
"""

#### Utility library #####
from netCDF4 import Dataset, chartostring
import numpy as np
import os
import matplotlib.pyplot as plt
import matplotlib.tri as mtri
#####################################################################################################
### Loading Cylinder in Crossflow DS ###
### link: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/G5MNYF&version=1.0
#####################################################################################################
# We use netCDF4 to open Exodus-II files (they are NetCDF under the hood).
try:
    from netCDF4 import Dataset, chartostring  # Dataset: file handle, chartostring: decode char arrays
except Exception as e:
    # Give a clear installation hint if netCDF4 is missing (handy for Colab).
    raise ImportError(
        "netCDF4 is required for Exodus-II files. Install with: pip install netCDF4\n"
        f"Original error: {e}"
    )
# We can use different file formats.
def name_scan(path: str) -> str | None:
    """
    Detect the file format from the filename only.

    Returns:
        'exodus' for .e/.exo files, otherwise None.

    Why:
        Keeping a simple detector lets you add more formats later without
        changing the caller code.
    """
    # Extract lowercase extension (e.g., '.e', '.exo', '.csv', etc.)
    ext = os.path.splitext(path)[1].lower()
    # Recognize Exodus-II by extension
    if ext in (".e", ".exo"):
        return "exodus"
    # Unknown for now (we only implement Exodus in this file)
    return None

def inspect_fields(path: str) -> dict:
    """
    Fast inspection of an Exodus-II file:
    - counts nodes/elements/time steps
    - lists nodal variable names (e.g., 'V_x', 'V_y', 'P')
    - suggests a likely time variable name

    Returns:
        dict with keys:
            'format'          : 'exodus'
            'num_nodes'       : int or None
            'num_elems'       : int or None
            'time_steps'      : int or None
            'nodal_var_names' : list[str]
            'time_key_guess'  : str or None
    """
    # Ensure the format is Exodus-II (this function is Exodus-only right now)
    fmt = name_scan(path)
    if fmt != "exodus":
        raise ValueError(f"Unsupported format for inspect_fields: {path}")

    # Open the NetCDF file (read-only)
    ds = Dataset(path, "r")
    # Avoid masked arrays (we prefer plain NumPy)
    ds.set_auto_mask(False)

    # Pull basic dimension sizes if present
    num_nodes = ds.dimensions["num_nodes"].size if "num_nodes" in ds.dimensions else None
    num_elems = ds.dimensions["num_elem"].size if "num_elem" in ds.dimensions else None
    time_steps = ds.dimensions["time_step"].size if "time_step" in ds.dimensions else None

    # Nodal variable names are stored in a char array 'name_nod_var'
    nodal_var_names = []
    if "name_nod_var" in ds.variables:
        # Convert NetCDF char array -> list[str]
        raw = chartostring(ds.variables["name_nod_var"][:]).tolist()
        # Some files pad with NULs; strip and keep clean strings
        nodal_var_names = [s.split("\x00", 1)[0].strip() for s in raw]

    # Guess a physical time variable:
    # any variable whose name contains 'time' and whose first dimension is 'time_step'
    candidates = [
        k for k, v in ds.variables.items()
        if "time" in k.lower() and getattr(v, "dimensions", ())[0] == "time_step"
    ]
    # Prefer canonical 'time_whole' if present; else first candidate; else None
    time_key_guess = "time_whole" if "time_whole" in candidates else (candidates[0] if candidates else None)

    # Always close the dataset handle
    ds.close()

    # Return a compact, useful summary
    return {
        "format": "exodus",
        "num_nodes": num_nodes,
        "num_elems": num_elems,
        "time_steps": time_steps,
        "nodal_var_names": nodal_var_names,
        "time_key_guess": time_key_guess,
    }


def load_fields(path, t_start: int = 0, t_count: int | None = None):
    """
    Load velocity (Vx, Vy) and pressure (P) from an Exodus-II file into
    (num_nodes, T) arrays, plus a stacked velocity array (2*num_nodes, T).

    Args:
        path     : str, path to *.e (Exodus-II file)
        t_start  : int, first time index to load (default 0)
        t_count  : int or None, number of time steps to load (None -> to end)

    Returns:
        times    : (T,)                    time vector (subset)
        coordx   : (num_nodes,)            x-coordinates per node
        coordy   : (num_nodes,)            y-coordinates per node
        Vx       : (num_nodes, T)          velocity-x over time
        Vy       : (num_nodes, T)          velocity-y over time
        P        : (num_nodes, T)          pressure over time
        V_stack  : (2*num_nodes, T)        stacked velocities [Vx; Vy] row-wise
    """
    # Guard: make sure we are handling an Exodus-II file
    fmt = name_scan(path)
    if fmt != "exodus":
        raise ValueError(f"Unsupported format for load_fields: {path}")

    # Open the NetCDF-backed Exodus file
    ds = Dataset(path, "r")
    # Avoid masked arrays for consistent shapes/dtypes
    ds.set_auto_mask(False)

    # --- Coordinates (standard Exodus names) ---
    coordx = ds.variables["coordx"][:]  # x-coordinates per node
    coordy = ds.variables["coordy"][:]  # y-coordinates per node
    num_nodes = coordx.size             # total number of nodes

    # --- Nodal variable names -> 1-based indices (Exodus convention) ---
    var_names = []  # will hold strings like 'V_x', 'V_y', 'P'
    if "name_nod_var" in ds.variables:
        raw = chartostring(ds.variables["name_nod_var"][:]).tolist()  # char array -> list[str]
        var_names = [s.split("\x00", 1)[0].strip() for s in raw]      # strip trailing NULs/spaces

    def _find_index(target: str) -> int | None:
        """Find the 1-based index for a nodal variable by case-insensitive exact match."""
        tl = target.lower()  # lowercase target
        for i, n in enumerate(var_names, start=1):  # exodus uses 1-based var numbering
            if n.lower() == tl:                     # case-insensitive equality
                return i                            # return 1-based index
        return None                                  # not found

    # Try the canonical names usually present in the dataset
    ix_vx = _find_index("V_x")  # 1-based index of V_x
    ix_vy = _find_index("V_y")  # 1-based index of V_y
    ix_p  = _find_index("P")    # 1-based index of P

    # Provide aliases as a robust fallback (different naming conventions)
    aliases = {
        "vx": ["v_x", "vx", "velocity_x", "u"],  # common alternatives for Vx
        "vy": ["v_y", "vy", "velocity_y", "v"],  # common alternatives for Vy
        "p" : ["p", "pressure"],                 # common alternatives for P
    }

    def _search_alias(cands: list[str]) -> int | None:
        """Try a list of candidate names; return the first found 1-based index."""
        for c in cands:
            j = _find_index(c)
            if j is not None:
                return j
        return None

    # If canonical names were not found, try aliases; as a last resort, default to typical indices
    ix_vx = ix_vx or _search_alias(aliases["vx"]) or 2  # fallback to 2 if needed
    ix_vy = ix_vy or _search_alias(aliases["vy"]) or 1  # fallback to 1 if needed
    ix_p  = ix_p  or _search_alias(aliases["p"])  or 3  # fallback to 3 if needed

    # --- Time indexing (robust) ---
    # Determine total available timesteps (prefer the 'time_step' dimension)
    if "time_step" in ds.dimensions:
        n_total = ds.dimensions["time_step"].size  # authoritative number of time steps
    else:
        # Rare fallback: infer time length from one of the nodal variables
        key_probe = f"vals_nod_var{ix_vx}"        # variable like 'vals_nod_var2'
        n_total = ds.variables[key_probe].shape[0] # first axis is time

    # Compute the time slice [t_start : t_end)
    if t_count is None:
        t_end = n_total                      # load to the end by default
    else:
        t_end = min(n_total, t_start + int(t_count))  # clamp to available range
    t_slice = slice(int(t_start), int(t_end))         # Python slice object
    T = t_end - t_start                               # number of time points to load

    # --- Extract fields (nodal variables) and transpose to (num_nodes, T) ---
    # Exodus stores nodal variables as ('time_step', 'num_nodes'), hence .T to get (nodes, time)
    Vx = ds.variables[f"vals_nod_var{ix_vx}"][t_slice, :].T  # velocity-x
    Vy = ds.variables[f"vals_nod_var{ix_vy}"][t_slice, :].T  # velocity-y
    P  = ds.variables[f"vals_nod_var{ix_p}" ][t_slice, :].T  # pressure

    # --- Physical time vector detection ---
    # Prefer 'time_whole', else any variable whose first dim is 'time_step' and name contains 'time'
    time_var = None
    candidates = [
        k for k, v in ds.variables.items()
        if "time" in k.lower() and getattr(v, "dimensions", ())[0] == "time_step"
    ]
    if "time_whole" in candidates:
        time_var = "time_whole"            # canonical Exodus time variable
    elif candidates:
        time_var = candidates[0]           # pick the first sensible candidate

    # Slice time according to t_slice; fall back to an index-based time if absent
    if time_var is None:
        times = np.arange(T, dtype=float)  # synthetic time: 0..T-1
    else:
        times = np.asarray(ds.variables[time_var][t_slice], dtype=float).reshape(-1)  # ensure 1D float

    # Close the dataset as we're done reading
    ds.close()

    # --- Convenience: stack velocities row-wise into shape (2*num_nodes, T) ---
    V_stack = np.vstack([Vx, Vy])  # [Vx; Vy]

    # --- Sanity checks on shapes to catch surprises early ---
    assert Vx.shape == (num_nodes, T)
    assert Vy.shape == (num_nodes, T)
    assert P.shape  == (num_nodes, T)
    assert V_stack.shape == (2 * num_nodes, T)
    assert times.shape == (T,)

    # Return the full bundle expected by your downstream code
    return times, coordx, coordy, Vx, Vy, P, V_stack


def plot_flow_snapshot(path, t_index=0, quiver_frac=0.03, probe_node=None):
    ds = Dataset(path, "r")
    x = ds.variables["coordx"][:]
    y = ds.variables["coordy"][:]

    conn_keys = [k for k in ds.variables.keys() if k.startswith("connect")]
    assert conn_keys, "No connectivity array ('connect*') found."
    conn = np.asarray(ds.variables[conn_keys[0]][:], dtype=int) - 1
    if conn.shape[1] == 3:
        tri_conn = conn
    elif conn.shape[1] == 4:
        tri_conn = np.vstack([conn[:, [0, 1, 2]],
                              conn[:, [0, 2, 3]]])
    else:
        raise ValueError(f"Unsupported element with {conn.shape[1]} nodes (expected 3 or 4).")
    tri = mtri.Triangulation(x, y, triangles=tri_conn)

    names = []
    if "name_nod_var" in ds.variables:
        names = [s.strip() for s in chartostring(ds.variables["name_nod_var"][:]).tolist()]
    def _idx(name, default):
        for i, n in enumerate(names, start=1):
            if n == name or n.lower() == name.lower():
                return i
        return default
    ix_vy = _idx("V_y", 1)
    ix_vx = _idx("V_x", 2)

    # Snapshot
    Vy_t = ds.variables[f"vals_nod_var{ix_vy}"][t_index, :]
    Vx_t = ds.variables[f"vals_nod_var{ix_vx}"][t_index, :]
    ds.close()

    speed = np.sqrt(Vx_t**2 + Vy_t**2)

    # Downsample for quiver
    n = x.size
    step = max(1, int(round(1.0 / max(quiver_frac, 1e-6))))
    q_idx = np.arange(0, n, step)

    # Figure: heatmap (|V|) + quiver
    fig, ax = plt.subplots(figsize=(15, 10))
    # Heatmap on unstructured mesh
    tpc = ax.tripcolor(tri, speed, shading="gouraud")
    cb = plt.colorbar(tpc, ax=ax, shrink=0.9, label="|V|")
    # Quiver on top
    ax.quiver(
        x[q_idx], y[q_idx], Vx_t[q_idx], Vy_t[q_idx],
        angles="xy", scale_units="xy", scale=1, width=0.002,
        alpha=0.9, zorder=3
    )
    ax.set_aspect("equal", adjustable="box")
    ax.set_title(f"{os.path.basename(path)} | t={t_index}")
    ax.set_xlabel("x"); ax.set_ylabel("y")
    fig.tight_layout()
    plt.show()

def plot_heatmap_contours(path, t_index=0, field="speed", levels=20):
    ds = Dataset(path, "r")
    x = ds.variables["coordx"][:]
    y = ds.variables["coordy"][:]

    conn_keys = [k for k in ds.variables.keys() if k.startswith("connect")]
    assert conn_keys, "No 'connect*' array found in file."
    conn = ds.variables[conn_keys[0]][:] - 1
    conn = np.asarray(conn, dtype=int)
    if conn.shape[1] == 3:
        tri_conn = conn
    elif conn.shape[1] == 4:
        tri_conn = np.vstack([conn[:, [0, 1, 2]],
                            conn[:, [0, 2, 3]]])
    else:
        raise ValueError(f"Unsupported element with {conn.shape[1]} nodes (expected 3 or 4).")

    tri = mtri.Triangulation(x, y, triangles=tri_conn)

    names = []
    if "name_nod_var" in ds.variables:
        names = [s.strip() for s in chartostring(ds.variables["name_nod_var"][:]).tolist()]
    def _idx(name, default):
        for i, n in enumerate(names, start=1):
            if n == name or n.lower() == name.lower():
                return i
        return default

    ix_vy = _idx("V_y", 1)
    ix_vx = _idx("V_x", 2)
    ix_p  = _idx("P",   3)

    Vx = ds.variables[f"vals_nod_var{ix_vx}"][t_index, :]
    Vy = ds.variables[f"vals_nod_var{ix_vy}"][t_index, :]
    P  = ds.variables[f"vals_nod_var{ix_p}"][t_index, :]
    ds.close()

    if field.lower() == "pressure":
        Z = P
        barlabel = "P"
        title_field = "Pressure"
    else:
        Z = np.sqrt(Vx**2 + Vy**2)
        barlabel = "|V|"
        title_field = "Speed"

    fig, ax = plt.subplots(figsize=(6.8, 4.8))
    tpc = ax.tripcolor(tri, Z, shading="gouraud")
    ax.tricontour(tri, Z, levels=levels, linewidths=2, alpha=1)
    ax.set_aspect("equal", adjustable="box")
    ax.set_xlabel("x"); ax.set_ylabel("y")
    ax.set_title(f"{os.path.basename(path)} | t={t_index} | {title_field}")
    plt.colorbar(tpc, ax=ax, label=barlabel)
    plt.tight_layout()
    plt.show()
